{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W3/ungraded_labs/C3_W3_Lab_3_Conv1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFiCyWQ-NC5D"
   },
   "source": [
    "# Ungraded Lab: Using Convolutional Neural Networks\n",
    "\n",
    "In this lab, you will look at another way of building your text classification model and this will be with a convolution layer. As you learned in Course 2 of this specialization, convolutions extract features by applying filters to the input. Let's see how you can use that for text data in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djvGxIRDHT5e"
   },
   "source": [
    "## Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y20Lud2ZMBhW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Download the subword encoded pretokenized dataset\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AW-4Vo4TMUHb"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Get the train and test splits\n",
    "train_data, test_data = dataset['train'], dataset['test'], \n",
    "\n",
    "# Shuffle the training data\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Batch and pad the datasets to the maximum length of the sequences\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfatNr6-IAcd"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "In Course 2, you were using 2D convolution layers because you were applying it on images. For temporal data such as text sequences, you will use [Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) instead so the convolution will happen over a single dimension. You will also append a pooling layer to reduce the output of the convolution layer. For this lab, you will use [GlobalMaxPooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) to get the max value across the time dimension. You can also use average pooling and you will do that in the next labs. See how these layers behave as standalone layers in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ay87qbqwIJaV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 1\n",
      "timesteps (sequence length): 20\n",
      "features (embedding size): 20\n",
      "filters: 128\n",
      "kernel_size: 5\n",
      "shape of input array: (1, 20, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of conv1d output: (1, 16, 128)\n",
      "shape of global max pooling output: (1, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "timesteps = 20\n",
    "features = 20\n",
    "filters = 128\n",
    "kernel_size = 5\n",
    "\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'timesteps (sequence length): {timesteps}')\n",
    "print(f'features (embedding size): {features}')\n",
    "print(f'filters: {filters}')\n",
    "print(f'kernel_size: {kernel_size}')\n",
    "\n",
    "# Define array input with random values\n",
    "random_input = np.random.rand(batch_size,timesteps,features)\n",
    "print(f'shape of input array: {random_input.shape}')\n",
    "\n",
    "# Pass array to convolution layer and inspect output shape\n",
    "conv1d = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')\n",
    "result = conv1d(random_input)\n",
    "print(f'shape of conv1d output: {result.shape}')\n",
    "\n",
    "# Pass array to max pooling layer and inspect output shape\n",
    "gmp = tf.keras.layers.GlobalMaxPooling1D()\n",
    "result = gmp(result)\n",
    "print(f'shape of global max pooling output: {result.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNNYF7tqO7it"
   },
   "source": [
    "You can build the model by simply appending the convolution and pooling layer after the embedding layer as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jo1jjO3vn0jo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          523840    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         41088     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 573249 (2.19 MB)\n",
      "Trainable params: 573249 (2.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "filters = 128\n",
    "kernel_size = 5\n",
    "dense_dim = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Uip7QOVzMoMq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLJu8HEvPG0L"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Training will take around 30 seconds per epoch and you will notice that it reaches higher accuracies than the previous models you've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7mlgzaRDMtF6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 81s 809ms/step - loss: 0.6156 - accuracy: 0.6556 - val_loss: 0.4180 - val_accuracy: 0.8105\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 78s 786ms/step - loss: 0.2979 - accuracy: 0.8754 - val_loss: 0.2786 - val_accuracy: 0.8823\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 77s 791ms/step - loss: 0.1598 - accuracy: 0.9427 - val_loss: 0.2751 - val_accuracy: 0.8882\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 87s 889ms/step - loss: 0.0765 - accuracy: 0.9802 - val_loss: 0.3007 - val_accuracy: 0.8858\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 99s 1s/step - loss: 0.0270 - accuracy: 0.9968 - val_loss: 0.3216 - val_accuracy: 0.8923\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 102s 1s/step - loss: 0.0092 - accuracy: 0.9996 - val_loss: 0.3510 - val_accuracy: 0.8922\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 91s 933ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 0.3754 - val_accuracy: 0.8911\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 98s 1s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 97s 993ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4105 - val_accuracy: 0.8906\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 97s 991ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4246 - val_accuracy: 0.8904\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp1Z7P9pYRSK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "# Plot the accuracy and results \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rD7ZS84PlUp"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this lab, you explored another model architecture you can use for text classification. In the next lessons, you will revisit full word encoding of the IMDB reviews and compare which model works best when the data is prepared that way."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W3_Lab_3_Conv1D.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
